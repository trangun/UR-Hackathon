{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_pickle('train_data_merged.pkl') # merged raw train dataset (77 entries)\n",
    "df_test = pd.read_pickle('test_data_merged.pkl') # merged raw test dataset (60 entries)\n",
    "X_imputed_v1 = pd.read_pickle('imputation_data_v1.pkl') # Imputation with train dataset alone \n",
    "X_imputed_v2 = pd.read_pickle('imputation_data_v2.pkl') # Imputation with train and test dataset together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation Selection\n",
    "\n",
    "The MSE of Lasso with the data that is imputed with train dataset alone is higher than that of Lasso with the one that is imputed with train and test dataset together.\n",
    "\n",
    "Thus, the imuptation with all data is a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mse of Lasso with v1 imputation is 2.523213165799959\n",
      "The mse of Lasso with v2 imputation is 2.2834710485317555\n"
     ]
    }
   ],
   "source": [
    "def reverse(y, mean, std):\n",
    "    return y * std + mean\n",
    "\n",
    "def mean_square_error(truth, y_pred):\n",
    "    return np.sqrt(np.sum((truth - y_pred)**2) / len(truth))\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "def validate(X, y, method):\n",
    "    loo = LeaveOneOut()\n",
    "    n_tryout = len(X)\n",
    "    s = 0\n",
    "    for train, test in loo.split(list(range(n_tryout))):\n",
    "        X_train = np.array(X.loc[train])\n",
    "        \n",
    "        X_test = np.array(X.loc[test])\n",
    "        y_test = np.array(y.loc[test])\n",
    "        \n",
    "        if method == 'log':\n",
    "            y_train = np.log(np.array(y.loc[train]))\n",
    "        elif method == 'normal':\n",
    "            mean = np.mean(np.array(y.loc[train]))\n",
    "            std = np.std(np.array(y.loc[train]))\n",
    "            y_train = (np.array(y.loc[train]) - mean) / std\n",
    "        elif method == 'None':\n",
    "            y_train = np.array(y.loc[train])\n",
    "        \n",
    "        scaler = preprocessing.StandardScaler().fit(X_train) # normalization\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        clf = linear_model.Lasso(alpha=0.1) # fit the lasso\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        if method == 'log':\n",
    "            s += mean_square_error(y_test, np.exp(clf.predict(X_test)))\n",
    "        elif method == 'normal':\n",
    "            s += mean_square_error(y_test, reverse(clf.predict(X_test), mean, std))\n",
    "        elif method == 'None':\n",
    "            s += mean_square_error(y_test, clf.predict(X_test))\n",
    "        \n",
    "    return s / n_tryout\n",
    "\n",
    "\n",
    "# The MSE of LASSO with v1 imputation \n",
    "print(\"The mse of Lasso with v1 imputation is %s\" % validate(X_imputed_v1, df_train['severity_score'], method = 'normal'))\n",
    "\n",
    "# The MSE of LASSO with v2 imputation \n",
    "print(\"The mse of Lasso with v2 imputation is %s\" % validate(X_imputed_v2.loc[:76], df_train['severity_score'], method = 'normal'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mse of TWO Lasso with v2 imputation is 1.2970142379825276\n",
      "Breakdown\n",
      "For ss > 2: 1.499125243015088\n",
      "For ss <= 2: 0.6345392770424688\n"
     ]
    }
   ],
   "source": [
    "df_cla = pd.concat([df_train[['severity_score']], X_imputed_v2.loc[:76]], axis = 1)\n",
    "df_cla['label'] = [1 if x > 2 else 0 for x in df_cla['severity_score']]\n",
    "\n",
    "df1 = df_cla[df_cla['label'] == 1].reset_index(drop = True)\n",
    "df2 = df_cla[df_cla['label'] == 0].reset_index(drop = True)\n",
    "\n",
    "score1 = validate(df1[list(df1)[1:-1]], df1['severity_score'], method = 'log')\n",
    "score2 = validate(df2[list(df2)[1:-1]], df2['severity_score'], method = 'log')\n",
    "score = (len(df1) * score1 + len(df2) * score2) / len(df_cla)\n",
    "\n",
    "print(\"The mse of TWO Lasso with v2 imputation is %s\" % score)\n",
    "print(\"Breakdown\")\n",
    "print(\"For ss > 2: %s\" % score1)\n",
    "print(\"For ss <= 2: %s\" % score2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Classification\n",
    "\n",
    "The performance of the classification is poor. The best model (SVM) only predicts all data as their severity scores are higher than 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Random Forest 0.7012987012987013\n",
      "SVM 0.7662337662337663\n",
      "Naive Bayes 0.6493506493506493\n",
      "Decision Trees 0.6233766233766234\n",
      "Linear Discriminant Analysis 0.7532467532467533\n"
     ]
    }
   ],
   "source": [
    "df_cla = pd.concat([df_train[['severity_score']], X_imputed_v2.loc[:76]], axis = 1)\n",
    "df_cla['label'] = [1 if x > 2 else 0 for x in df_cla['severity_score']]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def classify(X, y):\n",
    "    loo = LeaveOneOut()\n",
    "    n_tryout = len(X)\n",
    "    s_rf = 0\n",
    "    s_svm = 0\n",
    "    s_gnb = 0\n",
    "    s_tree = 0\n",
    "    s_lda = 0\n",
    "    for train, test in loo.split(list(range(n_tryout))):\n",
    "        X_train = np.array(X.loc[train])\n",
    "        y_train = np.array(y.loc[train])\n",
    "        X_test = np.array(X.loc[test])\n",
    "        y_test = np.array(y.loc[test])\n",
    "        \n",
    "        scaler = preprocessing.StandardScaler().fit(X_train) # normalization\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        s_rf += RandomForestClassifier(random_state=0).fit(X_train, y_train).score(X_test, y_test)\n",
    "        s_svm += svm.SVC(random_state=0).fit(X_train, y_train).score(X_test,  y_test)\n",
    "        s_gnb += GaussianNB().fit(X_train, y_train).score(X_test,  y_test)\n",
    "        s_tree += tree.DecisionTreeClassifier(random_state=0).fit(X_train, y_train).score(X_test,  y_test)\n",
    "        s_lda += LinearDiscriminantAnalysis().fit(X_train, y_train).score(X_test,  y_test)\n",
    "    \n",
    "    print(\"Accuracy\")\n",
    "    print(\"Random Forest %s\" % (s_rf / n_tryout))\n",
    "    print(\"SVM %s\" % (s_svm / n_tryout))\n",
    "    print(\"Naive Bayes %s\" % (s_gnb / n_tryout))\n",
    "    print(\"Decision Trees %s\" % (s_tree / n_tryout))\n",
    "    print(\"Linear Discriminant Analysis %s\" % (s_lda / n_tryout))\n",
    "\n",
    "classify(df_cla[list(df_cla)[1:-1]] , df_cla['label'])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Prediction for test dataset\n",
    "\n",
    "Normalize both X and y.\n",
    "Imputate with all data.\n",
    "Only one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, y_train, X_test):\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "\n",
    "    mean = np.mean(np.array(y_train))\n",
    "    std = np.std(np.array(y_train))\n",
    "    y_train = (np.array(y_train) - mean) / std\n",
    "        \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train) # normalization\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "        \n",
    "    clf = linear_model.Lasso(alpha=0.1) # fit the lasso\n",
    "    return reverse(clf.fit(X_train, y_train).predict(X_test), mean, std)\n",
    "        \n",
    "y_predict = predict(X_imputed_v2.loc[:76], df_train['severity_score'], X_imputed_v2.loc[77:])\n",
    "prediction = pd.read_csv('prediction.csv')\n",
    "prediction['severity_score'] = list(y_predict)\n",
    "prediction.to_csv('prediction_v0810.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
